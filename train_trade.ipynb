{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=2048, dataset_path='./cifar10', epoch=30000, imgc=3, imgsz=84, k_qry=15, k_spt=1, meta_lr=0.001, n_way=5, task_num=4, update_lr=0.01, update_step=5, update_step_test=10)\n",
      "=> no checkpoint found at 'mamltradesrseps2self1.pt'\n",
      "Meta(\n",
      "  (net): Learner(\n",
      "    conv2d:(ch_in:3, ch_out:32, k:3x3, stride:1, padding:0)\n",
      "    relu:(True,)\n",
      "    bn:(32,)\n",
      "    max_pool2d:(k:2, stride:2, padding:0)\n",
      "    conv2d:(ch_in:32, ch_out:32, k:3x3, stride:1, padding:0)\n",
      "    relu:(True,)\n",
      "    bn:(32,)\n",
      "    max_pool2d:(k:2, stride:2, padding:0)\n",
      "    conv2d:(ch_in:32, ch_out:32, k:3x3, stride:1, padding:0)\n",
      "    relu:(True,)\n",
      "    bn:(32,)\n",
      "    max_pool2d:(k:2, stride:2, padding:0)\n",
      "    conv2d:(ch_in:32, ch_out:32, k:3x3, stride:1, padding:0)\n",
      "    relu:(True,)\n",
      "    bn:(32,)\n",
      "    max_pool2d:(k:2, stride:1, padding:0)\n",
      "    flatten:()\n",
      "    linear:(in:800, out:5)\n",
      "    \n",
      "    (vars): ParameterList(\n",
      "        (0): Parameter containing: [torch.cuda.FloatTensor of size 32x3x3x3 (GPU 0)]\n",
      "        (1): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (2): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (3): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (4): Parameter containing: [torch.cuda.FloatTensor of size 32x32x3x3 (GPU 0)]\n",
      "        (5): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (6): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (7): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (8): Parameter containing: [torch.cuda.FloatTensor of size 32x32x3x3 (GPU 0)]\n",
      "        (9): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (10): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (11): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (12): Parameter containing: [torch.cuda.FloatTensor of size 32x32x3x3 (GPU 0)]\n",
      "        (13): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (14): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (15): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (16): Parameter containing: [torch.cuda.FloatTensor of size 5x800 (GPU 0)]\n",
      "        (17): Parameter containing: [torch.cuda.FloatTensor of size 5 (GPU 0)]\n",
      "    )\n",
      "    (vars_bn): ParameterList(\n",
      "        (0): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (1): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (2): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (3): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (4): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (5): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (6): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "        (7): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total trainable tensors: 32901\n",
      "shuffle DB :train, b:10000, 5-way, 1-shot, 15-query, resize:84\n",
      "shuffle DB :test, b:100, 5-way, 1-shot, 15-query, resize:84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renwang/.local/lib/python3.7/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 \ttraining acc: [0.22333333 0.24       0.24       0.23333333 0.24       0.24333333]\n",
      "step: 0 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renwang/META/MetaFT.py:376: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  corr_ind = (torch.eq(pred_q, y_qry) == True).nonzero()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: [0.2067 0.2286 0.2357 0.2383 0.2401 0.2396 0.2402 0.2402 0.2418 0.2424\n",
      " 0.2429]\n",
      "Test acc_adv: [0.001467 0.0012   0.001333 0.001333 0.001734 0.0016   0.001734 0.0016\n",
      " 0.001866 0.0016   0.0016  ]\n",
      "Test acc_adv_prior: [0.006973 0.004982 0.00536  0.00534  0.006752 0.00632  0.00682  0.006557\n",
      " 0.00757  0.00629  0.006123]\n",
      "step: 30 \ttraining acc: [0.22333333 0.36       0.35666667 0.36333333 0.36333333 0.36666667]\n",
      "step: 30 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 60 \ttraining acc: [0.20666667 0.31       0.31666667 0.32       0.31333333 0.31333333]\n",
      "step: 60 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 90 \ttraining acc: [0.21       0.37       0.39666667 0.40666667 0.40666667 0.39333333]\n",
      "step: 90 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 120 \ttraining acc: [0.22333333 0.30666667 0.31333333 0.31333333 0.27666667 0.28666667]\n",
      "step: 120 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 150 \ttraining acc: [0.14333333 0.29333333 0.29333333 0.29333333 0.29333333 0.29666667]\n",
      "step: 150 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 180 \ttraining acc: [0.15333333 0.22       0.22333333 0.20666667 0.2        0.19333333]\n",
      "step: 180 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 210 \ttraining acc: [0.19333333 0.35333333 0.37333333 0.37666667 0.38333333 0.37333333]\n",
      "step: 210 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 240 \ttraining acc: [0.17666667 0.27666667 0.3        0.30333333 0.31666667 0.32333333]\n",
      "step: 240 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 270 \ttraining acc: [0.13666667 0.34333333 0.36666667 0.36666667 0.37       0.35666667]\n",
      "step: 270 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 300 \ttraining acc: [0.15       0.26       0.29333333 0.29666667 0.29333333 0.29333333]\n",
      "step: 300 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 330 \ttraining acc: [0.18666667 0.25666667 0.26333333 0.31       0.29333333 0.30666667]\n",
      "step: 330 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 360 \ttraining acc: [0.21333333 0.24333333 0.30333333 0.32666667 0.33       0.33333333]\n",
      "step: 360 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 390 \ttraining acc: [0.27666667 0.28666667 0.30666667 0.30333333 0.30333333 0.31      ]\n",
      "step: 390 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 420 \ttraining acc: [0.16333333 0.21666667 0.26       0.24333333 0.26333333 0.26666667]\n",
      "step: 420 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 450 \ttraining acc: [0.19666667 0.3        0.3        0.31333333 0.32       0.33      ]\n",
      "step: 450 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 480 \ttraining acc: [0.21666667 0.27333333 0.29       0.30333333 0.31       0.31      ]\n",
      "step: 480 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "Test acc: [0.2072 0.2422 0.2429 0.247  0.2471 0.2489 0.2482 0.2484 0.2471 0.2489\n",
      " 0.2494]\n",
      "Test acc_adv: [0.06335 0.1028  0.1251  0.1364  0.1437  0.1498  0.1512  0.1537  0.1552\n",
      " 0.1575  0.1589 ]\n",
      "Test acc_adv_prior: [0.303  0.4182 0.506  0.546  0.576  0.5957 0.603  0.6123 0.6206 0.627\n",
      " 0.633 ]\n",
      "step: 510 \ttraining acc: [0.19666667 0.34       0.34666667 0.34333333 0.34666667 0.35333333]\n",
      "step: 510 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 540 \ttraining acc: [0.25       0.29666667 0.32       0.33666667 0.34666667 0.36      ]\n",
      "step: 540 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 570 \ttraining acc: [0.24       0.27333333 0.28       0.26666667 0.27333333 0.27333333]\n",
      "step: 570 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 600 \ttraining acc: [0.17333333 0.25666667 0.28666667 0.28666667 0.27       0.28333333]\n",
      "step: 600 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 630 \ttraining acc: [0.19       0.23       0.26666667 0.28       0.28       0.28      ]\n",
      "step: 630 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 660 \ttraining acc: [0.18666667 0.28333333 0.31666667 0.32       0.32       0.31333333]\n",
      "step: 660 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 690 \ttraining acc: [0.25666667 0.34333333 0.33       0.36666667 0.37       0.37      ]\n",
      "step: 690 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 720 \ttraining acc: [0.23       0.32666667 0.31666667 0.31       0.30666667 0.30333333]\n",
      "step: 720 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 750 \ttraining acc: [0.22333333 0.35       0.35333333 0.34666667 0.33333333 0.33333333]\n",
      "step: 750 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 780 \ttraining acc: [0.16666667 0.23       0.27666667 0.29       0.30666667 0.30333333]\n",
      "step: 780 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 810 \ttraining acc: [0.19666667 0.28       0.27       0.28       0.3        0.31333333]\n",
      "step: 810 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 840 \ttraining acc: [0.19       0.34333333 0.34       0.34333333 0.34666667 0.33333333]\n",
      "step: 840 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 870 \ttraining acc: [0.20666667 0.29666667 0.32       0.34       0.33666667 0.34      ]\n",
      "step: 870 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 900 \ttraining acc: [0.20333333 0.3        0.31666667 0.30333333 0.32333333 0.33      ]\n",
      "step: 900 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 930 \ttraining acc: [0.20333333 0.27       0.3        0.32       0.32333333 0.32666667]\n",
      "step: 930 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 960 \ttraining acc: [0.18       0.26666667 0.32       0.32333333 0.32333333 0.32333333]\n",
      "step: 960 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 990 \ttraining acc: [0.22333333 0.35       0.38666667 0.39       0.38       0.37333333]\n",
      "step: 990 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "Test acc: [0.2039 0.2341 0.2426 0.2437 0.2462 0.2472 0.2485 0.2498 0.2498 0.2505\n",
      " 0.2507]\n",
      "Test acc_adv: [0.07544 0.1132  0.1382  0.1509  0.1569  0.1614  0.1637  0.1653  0.1677\n",
      " 0.171   0.1718 ]\n",
      "Test acc_adv_prior: [0.37   0.4717 0.5635 0.615  0.635  0.6504 0.657  0.662  0.672  0.6836\n",
      " 0.6846]\n",
      "step: 1020 \ttraining acc: [0.25       0.27333333 0.28666667 0.28333333 0.30333333 0.3       ]\n",
      "step: 1020 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1050 \ttraining acc: [0.23       0.29       0.27       0.27333333 0.26666667 0.27333333]\n",
      "step: 1050 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1080 \ttraining acc: [0.17666667 0.28333333 0.32666667 0.33666667 0.34       0.34666667]\n",
      "step: 1080 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1110 \ttraining acc: [0.18       0.29       0.29       0.33       0.33       0.34333333]\n",
      "step: 1110 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1140 \ttraining acc: [0.2        0.38333333 0.41       0.40666667 0.40333333 0.4       ]\n",
      "step: 1140 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1170 \ttraining acc: [0.16       0.27       0.29666667 0.29666667 0.30666667 0.31333333]\n",
      "step: 1170 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1200 \ttraining acc: [0.15666667 0.29666667 0.32       0.33666667 0.33       0.34      ]\n",
      "step: 1200 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1230 \ttraining acc: [0.2        0.28333333 0.31333333 0.32       0.31666667 0.32333333]\n",
      "step: 1230 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1260 \ttraining acc: [0.21       0.23666667 0.25333333 0.27       0.27       0.27333333]\n",
      "step: 1260 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1290 \ttraining acc: [0.16       0.27       0.27666667 0.26333333 0.25333333 0.24666667]\n",
      "step: 1290 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1320 \ttraining acc: [0.18666667 0.23       0.23       0.24       0.23666667 0.24666667]\n",
      "step: 1320 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1350 \ttraining acc: [0.2        0.23666667 0.26       0.26666667 0.28333333 0.28333333]\n",
      "step: 1350 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1380 \ttraining acc: [0.18333333 0.26333333 0.28       0.29       0.29333333 0.29666667]\n",
      "step: 1380 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1410 \ttraining acc: [0.25333333 0.36       0.35666667 0.36333333 0.36666667 0.37666667]\n",
      "step: 1410 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1440 \ttraining acc: [0.20333333 0.27       0.28       0.29666667 0.31333333 0.32333333]\n",
      "step: 1440 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1470 \ttraining acc: [0.19666667 0.25333333 0.26666667 0.27666667 0.29333333 0.31666667]\n",
      "step: 1470 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1500 \ttraining acc: [0.19       0.3        0.36666667 0.36666667 0.36333333 0.36      ]\n",
      "step: 1500 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "Test acc: [0.198  0.2341 0.2532 0.2544 0.2603 0.2605 0.2607 0.2612 0.2593 0.261\n",
      " 0.26  ]\n",
      "Test acc_adv: [0.1023 0.1204 0.1476 0.1587 0.1643 0.1697 0.1752 0.1752 0.1775 0.1803\n",
      " 0.1802]\n",
      "Test acc_adv_prior: [0.514  0.5054 0.5825 0.6196 0.6313 0.651  0.6694 0.6685 0.6836 0.6924\n",
      " 0.6943]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1530 \ttraining acc: [0.14       0.26       0.28333333 0.32       0.33333333 0.33333333]\n",
      "step: 1530 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1560 \ttraining acc: [0.16666667 0.23666667 0.25333333 0.26666667 0.26       0.26666667]\n",
      "step: 1560 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1590 \ttraining acc: [0.16       0.22666667 0.26       0.27       0.27333333 0.26      ]\n",
      "step: 1590 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1620 \ttraining acc: [0.16666667 0.28       0.32       0.33       0.32       0.31333333]\n",
      "step: 1620 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1650 \ttraining acc: [0.24       0.29333333 0.31666667 0.31       0.30333333 0.30666667]\n",
      "step: 1650 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1680 \ttraining acc: [0.10666667 0.29333333 0.34333333 0.37       0.38666667 0.38      ]\n",
      "step: 1680 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1710 \ttraining acc: [0.15       0.23666667 0.24333333 0.25333333 0.26333333 0.26666667]\n",
      "step: 1710 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1740 \ttraining acc: [0.14333333 0.25666667 0.29666667 0.29       0.28       0.29      ]\n",
      "step: 1740 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1770 \ttraining acc: [0.29333333 0.34       0.34       0.35       0.37333333 0.37333333]\n",
      "step: 1770 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1800 \ttraining acc: [0.18666667 0.21       0.22666667 0.22666667 0.22333333 0.22      ]\n",
      "step: 1800 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1830 \ttraining acc: [0.22666667 0.34       0.33666667 0.35       0.34333333 0.34333333]\n",
      "step: 1830 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1860 \ttraining acc: [0.24       0.31666667 0.33       0.33333333 0.32666667 0.32      ]\n",
      "step: 1860 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1890 \ttraining acc: [0.26666667 0.27       0.33666667 0.34333333 0.32333333 0.31666667]\n",
      "step: 1890 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1920 \ttraining acc: [0.15333333 0.26       0.31       0.32666667 0.33333333 0.33      ]\n",
      "step: 1920 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1950 \ttraining acc: [0.22333333 0.34333333 0.35333333 0.35333333 0.36666667 0.36333333]\n",
      "step: 1950 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 1980 \ttraining acc: [0.19333333 0.21333333 0.23       0.24333333 0.25       0.25333333]\n",
      "step: 1980 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "Test acc: [0.1908 0.2361 0.2625 0.2654 0.2666 0.2698 0.2712 0.2725 0.2725 0.272\n",
      " 0.2715]\n",
      "Test acc_adv: [0.05774 0.0897  0.131   0.1533  0.1653  0.1725  0.1764  0.1775  0.1794\n",
      " 0.1824  0.1837 ]\n",
      "Test acc_adv_prior: [0.2964 0.3716 0.4897 0.5713 0.6133 0.632  0.6416 0.6426 0.651  0.663\n",
      " 0.6694]\n",
      "step: 2010 \ttraining acc: [0.22666667 0.25333333 0.23666667 0.23333333 0.22666667 0.23      ]\n",
      "step: 2010 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2040 \ttraining acc: [0.17       0.22333333 0.24666667 0.28333333 0.28       0.29333333]\n",
      "step: 2040 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2070 \ttraining acc: [0.15       0.28666667 0.37666667 0.36666667 0.36666667 0.36      ]\n",
      "step: 2070 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2100 \ttraining acc: [0.15333333 0.27666667 0.33666667 0.36333333 0.38666667 0.4       ]\n",
      "step: 2100 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2130 \ttraining acc: [0.23333333 0.29333333 0.32666667 0.35333333 0.34333333 0.35333333]\n",
      "step: 2130 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2160 \ttraining acc: [0.21333333 0.3        0.32       0.32333333 0.32333333 0.33      ]\n",
      "step: 2160 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2190 \ttraining acc: [0.18666667 0.28666667 0.29333333 0.33       0.34333333 0.35      ]\n",
      "step: 2190 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2220 \ttraining acc: [0.21       0.27666667 0.29666667 0.33       0.35333333 0.36666667]\n",
      "step: 2220 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2250 \ttraining acc: [0.23       0.31333333 0.34666667 0.35666667 0.35666667 0.34666667]\n",
      "step: 2250 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2280 \ttraining acc: [0.24666667 0.26666667 0.29       0.30666667 0.32333333 0.33666667]\n",
      "step: 2280 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2310 \ttraining acc: [0.2        0.25       0.33666667 0.35666667 0.35       0.34666667]\n",
      "step: 2310 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2340 \ttraining acc: [0.17       0.26666667 0.36       0.37       0.37666667 0.39666667]\n",
      "step: 2340 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2370 \ttraining acc: [0.20333333 0.31       0.40333333 0.39333333 0.37       0.37333333]\n",
      "step: 2370 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2400 \ttraining acc: [0.18       0.32666667 0.37       0.39       0.36333333 0.37      ]\n",
      "step: 2400 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2430 \ttraining acc: [0.25333333 0.34666667 0.34333333 0.34       0.33666667 0.34      ]\n",
      "step: 2430 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2460 \ttraining acc: [0.13666667 0.18       0.23666667 0.26       0.26333333 0.26      ]\n",
      "step: 2460 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 2490 \ttraining acc: [0.16666667 0.27333333 0.29666667 0.30333333 0.31666667 0.32666667]\n",
      "step: 2490 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 0 \ttraining acc: [0.26       0.32666667 0.38       0.37666667 0.38333333 0.39      ]\n",
      "step: 0 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "Test acc: [0.2004 0.2386 0.2654 0.2764 0.28   0.283  0.2842 0.2837 0.285  0.2854\n",
      " 0.2883]\n",
      "Test acc_adv: [0.06305 0.0944  0.12494 0.1497  0.166   0.1759  0.1803  0.1849  0.1874\n",
      " 0.1898  0.1912 ]\n",
      "Test acc_adv_prior: [0.3105 0.3892 0.4656 0.536  0.587  0.6157 0.628  0.6465 0.654  0.661\n",
      " 0.66  ]\n",
      "step: 30 \ttraining acc: [0.17666667 0.26333333 0.32333333 0.33       0.33       0.35      ]\n",
      "step: 30 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 60 \ttraining acc: [0.19       0.25333333 0.27333333 0.30333333 0.31       0.31666667]\n",
      "step: 60 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 90 \ttraining acc: [0.23       0.44       0.47       0.47666667 0.48       0.48      ]\n",
      "step: 90 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 120 \ttraining acc: [0.19666667 0.26666667 0.35666667 0.34666667 0.35       0.34666667]\n",
      "step: 120 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 150 \ttraining acc: [0.21333333 0.30333333 0.32333333 0.33333333 0.34666667 0.35666667]\n",
      "step: 150 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 180 \ttraining acc: [0.20666667 0.25333333 0.28333333 0.27       0.28666667 0.29333333]\n",
      "step: 180 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 210 \ttraining acc: [0.32333333 0.38666667 0.39333333 0.38       0.37       0.36666667]\n",
      "step: 210 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 240 \ttraining acc: [0.21666667 0.26666667 0.31666667 0.32666667 0.33333333 0.33666667]\n",
      "step: 240 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 270 \ttraining acc: [0.21666667 0.29       0.31666667 0.34333333 0.35       0.34666667]\n",
      "step: 270 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 300 \ttraining acc: [0.17333333 0.28333333 0.33666667 0.34       0.35666667 0.36333333]\n",
      "step: 300 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 330 \ttraining acc: [0.24666667 0.37       0.44666667 0.46333333 0.46666667 0.47666667]\n",
      "step: 330 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 360 \ttraining acc: [0.20333333 0.28666667 0.32       0.33       0.32666667 0.33333333]\n",
      "step: 360 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 390 \ttraining acc: [0.18666667 0.25333333 0.30666667 0.3        0.30666667 0.29      ]\n",
      "step: 390 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 420 \ttraining acc: [0.18333333 0.25       0.28333333 0.30333333 0.31333333 0.31666667]\n",
      "step: 420 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 450 \ttraining acc: [0.16666667 0.24       0.26333333 0.26333333 0.27666667 0.28      ]\n",
      "step: 450 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 480 \ttraining acc: [0.23       0.27666667 0.31       0.32666667 0.33666667 0.35333333]\n",
      "step: 480 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "Test acc: [0.1931 0.2374 0.2676 0.2847 0.2888 0.291  0.2937 0.295  0.2993 0.2998\n",
      " 0.3013]\n",
      "Test acc_adv: [0.0895 0.1129 0.1422 0.1613 0.1744 0.1796 0.1855 0.1904 0.1925 0.194\n",
      " 0.1953]\n",
      "Test acc_adv_prior: [0.452  0.4634 0.5176 0.5557 0.596  0.614  0.628  0.643  0.6396 0.642\n",
      " 0.643 ]\n",
      "step: 510 \ttraining acc: [0.21       0.27       0.33666667 0.32       0.33333333 0.34      ]\n",
      "step: 510 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 540 \ttraining acc: [0.17       0.32333333 0.38333333 0.41666667 0.42       0.43666667]\n",
      "step: 540 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 570 \ttraining acc: [0.21       0.36666667 0.38       0.39       0.4        0.4       ]\n",
      "step: 570 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 600 \ttraining acc: [0.22       0.32666667 0.37333333 0.4        0.40666667 0.41666667]\n",
      "step: 600 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 630 \ttraining acc: [0.27333333 0.33666667 0.40666667 0.42333333 0.42666667 0.43666667]\n",
      "step: 630 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 660 \ttraining acc: [0.20333333 0.25333333 0.28666667 0.32666667 0.35       0.36333333]\n",
      "step: 660 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 690 \ttraining acc: [0.2        0.36       0.4        0.39666667 0.39333333 0.38333333]\n",
      "step: 690 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 720 \ttraining acc: [0.21333333 0.23333333 0.26333333 0.28333333 0.3        0.3       ]\n",
      "step: 720 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 750 \ttraining acc: [0.2        0.27333333 0.29666667 0.32666667 0.33       0.35333333]\n",
      "step: 750 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 780 \ttraining acc: [0.21       0.36       0.44       0.45       0.45666667 0.47333333]\n",
      "step: 780 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 810 \ttraining acc: [0.18333333 0.25       0.30333333 0.31       0.30666667 0.31333333]\n",
      "step: 810 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 840 \ttraining acc: [0.23333333 0.3        0.30333333 0.33       0.34333333 0.34666667]\n",
      "step: 840 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 870 \ttraining acc: [0.15333333 0.23       0.27       0.29666667 0.30666667 0.30333333]\n",
      "step: 870 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 900 \ttraining acc: [0.21333333 0.30333333 0.34       0.39666667 0.42       0.43333333]\n",
      "step: 900 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 930 \ttraining acc: [0.15666667 0.24666667 0.28333333 0.32666667 0.33333333 0.34      ]\n",
      "step: 930 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 960 \ttraining acc: [0.22333333 0.23666667 0.29666667 0.3        0.29666667 0.29333333]\n",
      "step: 960 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 990 \ttraining acc: [0.19333333 0.28666667 0.35333333 0.35666667 0.36666667 0.36666667]\n",
      "step: 990 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "Test acc: [0.1912 0.2483 0.2817 0.2961 0.3037 0.3096 0.312  0.314  0.315  0.316\n",
      " 0.3157]\n",
      "Test acc_adv: [0.0688 0.1127 0.1512 0.1733 0.1876 0.1935 0.1976 0.1998 0.2026 0.2042\n",
      " 0.2048]\n",
      "Test acc_adv_prior: [0.3496 0.445  0.529  0.5796 0.614  0.621  0.629  0.6323 0.639  0.642\n",
      " 0.6445]\n",
      "step: 1020 \ttraining acc: [0.19       0.27       0.31333333 0.30666667 0.31666667 0.32      ]\n",
      "step: 1020 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import  torch, os\n",
    "import  numpy as np\n",
    "from    MiniImagenet import MiniImagenet\n",
    "import  scipy.stats\n",
    "from    torch.utils.data import DataLoader\n",
    "from    torch.optim import lr_scheduler\n",
    "import  random, sys, pickle\n",
    "import  argparse\n",
    "import skimage.transform\n",
    "\n",
    "from LoadUnlableData import UnlabData\n",
    "\n",
    "#from MAMLROBUST import Meta\n",
    "# import dataset_input\n",
    "# import utilities\n",
    "import time\n",
    "from MetaFT import Meta\n",
    "\n",
    "\n",
    "\n",
    "def mean_confidence_interval(accs, confidence=0.95):\n",
    "    n = accs.shape[0]\n",
    "    m, se = np.mean(accs), scipy.stats.sem(accs)\n",
    "    h = se * scipy.stats.t._ppf((1 + confidence) / 2, n - 1)\n",
    "    return m, h\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    torch.manual_seed(222)\n",
    "    torch.cuda.manual_seed_all(222)\n",
    "    np.random.seed(222)\n",
    "\n",
    "    print(args)\n",
    "\n",
    "    config = [\n",
    "        ('conv2d', [32, 3, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 1, 0]),\n",
    "        ('flatten', []),\n",
    "        ('linear', [args.n_way, 32 * 5 * 5])\n",
    "    ]\n",
    "\n",
    "    device = torch.device('cuda:0')\n",
    "    maml = Meta(args, config, device).to(device)\n",
    "#     maml = Meta(args, config)\n",
    "\n",
    "    start_epoch = 0\n",
    "    start_step = 0\n",
    "    filename = 'mamltradesrseps2self1.pt'\n",
    "    #maml = Meta(args, config).to(device)\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        start_step = checkpoint['step']\n",
    "        maml.net.load_state_dict(checkpoint['state_dict'])\n",
    "        #maml = maml.to(device)\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "        #maml = Meta(args, config).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    tmp = filter(lambda x: x.requires_grad, maml.parameters())\n",
    "    num = sum(map(lambda x: np.prod(x.shape), tmp))\n",
    "    print(maml)\n",
    "    print('Total trainable tensors:', num)\n",
    "\n",
    "    # batchsz here means total episode number\n",
    "    mini = MiniImagenet('../', mode='train', n_way=args.n_way, k_shot=args.k_spt,\n",
    "                        k_query=args.k_qry,\n",
    "                        batchsz=10000, resize=args.imgsz)\n",
    "    mini_test = MiniImagenet('../', mode='test', n_way=args.n_way, k_shot=args.k_spt,\n",
    "                             k_query=args.k_qry,\n",
    "                             batchsz=100, resize=args.imgsz)\n",
    "    \n",
    "    tinyimg = UnlabData()\n",
    "    batchsiz = 20\n",
    "    \n",
    "    \n",
    "\n",
    "    for epoch in range(start_epoch, args.epoch//10000):\n",
    "        # fetch meta_batchsz num of episode each time\n",
    "        db = DataLoader(mini, args.task_num, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        \n",
    "            \n",
    "\n",
    "        for step, (x_spt, y_spt, x_qry, y_qry) in enumerate(db):\n",
    "#             if step == 1:\n",
    "#                 t = time.perf_counter()\n",
    "#             if step == 499:\n",
    "#                 ExecTime = time.perf_counter() - t\n",
    "#                 print(ExecTime)\n",
    "            x_unlab = torch.zeros((args.task_num, args.n_way, batchsiz, 3, 84, 84))\n",
    "\n",
    "            x_spt, y_spt, x_qry, y_qry = x_spt.to(device), y_spt.to(device), x_qry.to(device), y_qry.to(device)\n",
    "            \n",
    "            for i in range(args.task_num):\n",
    "                for j in range(args.n_way):\n",
    "                    index = y_spt[i][j].cpu().numpy()\n",
    "                    temp_train = tinyimg.train_data[index].get_next_batch(batchsiz,multiple_passes=True)\n",
    "                    \n",
    "                    x_unlab[i][j]= torch.from_numpy(temp_train.astype(np.float32))\n",
    "            x_unlab = x_unlab.to(device)\n",
    "            \n",
    "\n",
    "            accs, accs_adv = maml(x_spt, y_spt, x_qry, y_qry, x_unlab)\n",
    "\n",
    "            if step % 30 == 0:\n",
    "                print('step:', step, '\\ttraining acc:', accs)\n",
    "                print('step:', step, '\\ttraining acc_adv:', accs_adv)\n",
    "                state = {'epoch': epoch, 'step': step, 'state_dict': maml.net.state_dict()}\n",
    "                #torch.save(state, 'mamltradesrseps2self.pt')\n",
    "\n",
    "            if step % 500 == 0 or step % 2500 == 0:  # evaluation\n",
    "                db_test = DataLoader(mini_test, 1, shuffle=True, num_workers=0, pin_memory=True)\n",
    "                accs_all_test = []\n",
    "                accsadv_all_test = []\n",
    "                accsadvpr_all_test = []\n",
    "\n",
    "                for x_spt, y_spt, x_qry, y_qry in db_test:\n",
    "                    x_spt, y_spt, x_qry, y_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                                 x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device)\n",
    "#                     x_spt, y_spt, x_qry, y_qry = x_spt.squeeze(0), y_spt.squeeze(0), \\\n",
    "#                                                  x_qry.squeeze(0), y_qry.squeeze(0)\n",
    "\n",
    "                    accs, accs_adv, accs_adv_prior = maml.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "                    accs_all_test.append(accs)\n",
    "                    accsadv_all_test.append(accs_adv)\n",
    "                    accsadvpr_all_test.append(accs_adv_prior)\n",
    "\n",
    "                # [b, update_step+1]\n",
    "                accs = np.array(accs_all_test).mean(axis=0).astype(np.float16)\n",
    "                accs_adv = np.array(accsadv_all_test).mean(axis=0).astype(np.float16)\n",
    "                accs_adv_prior = np.array(accsadvpr_all_test).mean(axis=0).astype(np.float16)\n",
    "                print('Test acc:', accs)\n",
    "                print('Test acc_adv:', accs_adv)\n",
    "                print('Test acc_adv_prior:', accs_adv_prior)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    argparser = argparse.ArgumentParser()\n",
    "    argparser.add_argument('--epoch', type=int, help='epoch number', default=30000)\n",
    "    argparser.add_argument('--n_way', type=int, help='n way', default=5)\n",
    "    argparser.add_argument('--k_spt', type=int, help='k shot for support set', default=1)\n",
    "    argparser.add_argument('--k_qry', type=int, help='k shot for query set', default=15)\n",
    "    argparser.add_argument('--imgsz', type=int, help='imgsz', default=84)\n",
    "    argparser.add_argument('--imgc', type=int, help='imgc', default=3)\n",
    "    argparser.add_argument('--task_num', type=int, help='meta batch size, namely task num', default=4)\n",
    "    argparser.add_argument('--meta_lr', type=float, help='meta-level outer learning rate', default=1e-3)\n",
    "    argparser.add_argument('--update_lr', type=float, help='task-level inner update learning rate', default=0.01)\n",
    "    argparser.add_argument('--update_step', type=int, help='task-level inner update steps', default=5)\n",
    "    argparser.add_argument('--update_step_test', type=int, help='update steps for finetunning', default=10)\n",
    "    argparser.add_argument('--batch-size', default=2048, type=int, help='batch size')\n",
    "    argparser.add_argument('--dataset-path', default = './cifar10', type=str, help='dataset folder')\n",
    "    \n",
    "    #argparser.add_argument('--fast', action=\"store_true\", help='whether to use fgsm')\n",
    "\n",
    "    args = argparser.parse_args(args=[])\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
