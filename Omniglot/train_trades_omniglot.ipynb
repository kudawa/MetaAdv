{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch, os\n",
    "import  numpy as np\n",
    "import  scipy.stats\n",
    "from    torch.utils.data import DataLoader\n",
    "from    torch.optim import lr_scheduler\n",
    "import  random, sys, pickle\n",
    "import  argparse\n",
    "\n",
    "\n",
    "from torchmeta.datasets import Omniglot\n",
    "from torchmeta.transforms import Categorical, ClassSplitter, Rotation\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "\n",
    "from MetaFTOmni import Meta\n",
    "\n",
    "\n",
    "\n",
    "def mean_confidence_interval(accs, confidence=0.95):\n",
    "    n = accs.shape[0]\n",
    "    m, se = np.mean(accs), scipy.stats.sem(accs)\n",
    "    h = se * scipy.stats.t._ppf((1 + confidence) / 2, n - 1)\n",
    "    return m, h\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    torch.manual_seed(222)\n",
    "    torch.cuda.manual_seed_all(222)\n",
    "    np.random.seed(222)\n",
    "\n",
    "    print(args)\n",
    "\n",
    "    config = [\n",
    "        ('conv2d', [64, 1, 3, 3, 2, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [64]),\n",
    "        ('conv2d', [64, 64, 3, 3, 2, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [64]),\n",
    "        ('conv2d', [64, 64, 3, 3, 2, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [64]),\n",
    "        ('conv2d', [64, 64, 2, 2, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [64]),\n",
    "        ('flatten', []),\n",
    "        ('linear', [args.n_way, 64])\n",
    "    ]\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    best_cl = 0\n",
    "    best_rb = 0\n",
    "    filename = 'mamltrades_eps10_omniglot_5way.pt'\n",
    "    maml = Meta(args, config, device).to(device)\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        best_cl = checkpoint['cl']\n",
    "        best_rb = checkpoint['rb']\n",
    "        maml.net.load_state_dict(checkpoint['state_dict'])\n",
    "        #maml = maml.to(device)\n",
    "#         print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "#                   .format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    tmp = filter(lambda x: x.requires_grad, maml.parameters())\n",
    "    num = sum(map(lambda x: np.prod(x.shape), tmp))\n",
    "    print(maml)\n",
    "    print('Total trainable tensors:', num)\n",
    "\n",
    "    # batchsz here means total episode number\n",
    "    \n",
    "    print(best_cl)\n",
    "    print(best_rb)\n",
    "    \n",
    "\n",
    "    data_train = Omniglot(\"data\",\n",
    "                   # Number of ways\n",
    "                   num_classes_per_task=args.n_way,\n",
    "                   meta_train=True,\n",
    "                   meta_val=False,\n",
    "                   meta_test=False,\n",
    "                   # Resize the images to 28x28 and converts them to PyTorch tensors (from Torchvision)\n",
    "                   transform=Compose([Resize(28), ToTensor()]),\n",
    "                   # Transform the labels to integers (e.g. (\"Glagolitic/character01\", \"Sanskrit/character14\", ...) to (0, 1, ...))\n",
    "                   target_transform=Categorical(num_classes=args.n_way),\n",
    "                   # Creates new virtual classes with rotated versions of the images (from Santoro et al., 2016)\n",
    "                   #class_augmentations=[Rotation([90, 180, 270])],\n",
    "                   download=True)\n",
    "    \n",
    "    data_test = Omniglot(\"data\",\n",
    "                   # Number of ways\n",
    "                   num_classes_per_task=args.n_way,\n",
    "                   meta_train=False,\n",
    "                   meta_val=False,\n",
    "                   meta_test=True,\n",
    "                   # Resize the images to 28x28 and converts them to PyTorch tensors (from Torchvision)\n",
    "                   transform=Compose([Resize(28), ToTensor()]),\n",
    "                   # Transform the labels to integers (e.g. (\"Glagolitic/character01\", \"Sanskrit/character14\", ...) to (0, 1, ...))\n",
    "                   target_transform=Categorical(num_classes=args.n_way),\n",
    "                   # Creates new virtual classes with rotated versions of the images (from Santoro et al., 2016)\n",
    "                   #class_augmentations=[Rotation([90, 180, 270])],\n",
    "                   download=True)\n",
    "    data_train = ClassSplitter(data_train, shuffle=True, num_train_per_class=args.k_spt, num_test_per_class=args.k_qry)\n",
    "    data_test = ClassSplitter(data_test, shuffle=True, num_train_per_class=args.k_spt, num_test_per_class=args.k_qry)\n",
    "    \n",
    "    LST = None #UnlabData()\n",
    "    batchsiz = 20\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for epoch in range(args.epoch//10000):\n",
    "        # fetch meta_batchsz num of episode each time\n",
    "        db = BatchMetaDataLoader(data_train, batch_size=args.task_num, num_workers=0)\n",
    "        #db = DataLoader(data_train, args.task_num, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "        for step, batch_train in enumerate(db):\n",
    "        #for step, (x_spt, y_spt, x_qry, y_qry) in enumerate(db):\n",
    "            \n",
    "\n",
    "\n",
    "            x_spt, y_spt = batch_train[\"train\"]\n",
    "            x_qry, y_qry = batch_train[\"test\"]\n",
    "\n",
    "            x_spt, y_spt, x_qry, y_qry = x_spt.to(device), y_spt.to(device), x_qry.to(device), y_qry.to(device)\n",
    "            \n",
    "\n",
    "            \n",
    "            if LST:\n",
    "                x_unlab = torch.zeros((args.task_num, args.n_way, batchsiz, 3, 32, 32))\n",
    "                for i in range(args.task_num):                   \n",
    "                    with torch.no_grad():\n",
    "                        *_, outputs = resmodel(x_spt[i])\n",
    "                        _, y_unlab = outputs.max(1)\n",
    "                    index = y_unlab.cpu().numpy()\n",
    "                    for j in range(args.n_way):\n",
    "                        temp_train = LST.train_data[index[j]].get_next_batch(batchsiz,multiple_passes=True)\n",
    "                        x_unlab[i][j]= torch.from_numpy(temp_train.astype(np.float32))\n",
    "                x_unlab = x_unlab.to(device)\n",
    "            else:\n",
    "                x_unlab = None\n",
    "            \n",
    "\n",
    "            accs, accs_adv = maml(x_spt, y_spt, x_qry, y_qry, x_unlab)\n",
    "            \n",
    "            if step % 5000 == 0:\n",
    "                print('step:', step, '\\ttraining acc:', accs)\n",
    "                print('step:', step, '\\ttraining acc_adv:', accs_adv)\n",
    "                \n",
    "\n",
    "            if step % 10000== 0: # evaluation\n",
    "                #db_test = DataLoader(data_test, 1, shuffle=True, num_workers=0, pin_memory=True)\n",
    "                db_test = BatchMetaDataLoader(data_test, batch_size=1, num_workers=0)\n",
    "                accs_all_test = []\n",
    "                accsadv_all_test = []\n",
    "                accsadvpr_all_test = []\n",
    "\n",
    "                for step_t, batch_test in enumerate(db_test):\n",
    "                    x_spt, y_spt = batch_test[\"train\"]\n",
    "                    x_qry, y_qry = batch_test[\"test\"]\n",
    "\n",
    "\n",
    "                    x_spt, y_spt, x_qry, y_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                                 x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device)\n",
    "\n",
    "                    accs, accs_adv, accs_adv_prior = maml.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "                    accs_all_test.append(accs)\n",
    "                    accsadv_all_test.append(accs_adv)\n",
    "                    accsadvpr_all_test.append(accs_adv_prior)\n",
    "                    if step_t == 10000:\n",
    "                        break\n",
    "\n",
    "                # [b, update_step+1]\n",
    "                accs = np.array(accs_all_test).mean(axis=0).astype(np.float16)\n",
    "                accs_adv = np.array(accsadv_all_test).mean(axis=0).astype(np.float16)\n",
    "                accs_adv_prior = np.array(accsadvpr_all_test).mean(axis=0).astype(np.float16)\n",
    "                print('Test acc:', accs)\n",
    "                print('Test acc_adv:', accs_adv)\n",
    "                print('Test acc_adv_prior:', accs_adv_prior)\n",
    "\n",
    "                if best_cl < accs[-1] and best_rb < accs_adv[-1]:\n",
    "                    best_cl = accs[-1]\n",
    "                    best_rb = accs_adv[-1]\n",
    "                    state = {'cl': best_cl, 'rb': best_rb, 'state_dict': maml.net.state_dict()}\n",
    "                    torch.save(state, 'mamltrades_eps10_omniglot_5way.pt')\n",
    "                    print(best_cl)\n",
    "                    print(best_rb)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    argparser = argparse.ArgumentParser()\n",
    "    argparser.add_argument('--epoch', type=int, help='epoch number', default=30000)\n",
    "    argparser.add_argument('--n_way', type=int, help='n way', default=5)\n",
    "    argparser.add_argument('--k_spt', type=int, help='k shot for support set', default=1)\n",
    "    argparser.add_argument('--k_qry', type=int, help='k shot for query set', default=15)\n",
    "    argparser.add_argument('--imgsz', type=int, help='imgsz', default=32)\n",
    "    argparser.add_argument('--imgc', type=int, help='imgc', default=3)\n",
    "    argparser.add_argument('--task_num', type=int, help='meta batch size, namely task num', default=4)\n",
    "    argparser.add_argument('--meta_lr', type=float, help='meta-level outer learning rate', default=1e-3)\n",
    "    argparser.add_argument('--update_lr', type=float, help='task-level inner update learning rate', default=0.01)\n",
    "    argparser.add_argument('--update_step', type=int, help='task-level inner update steps', default=5)\n",
    "    argparser.add_argument('--update_step_test', type=int, help='update steps for finetunning', default=10)\n",
    "    \n",
    "    #argparser.add_argument('--fast', action=\"store_true\", help='whether to use fgsm')\n",
    "\n",
    "    args = argparser.parse_args(args=[])\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
