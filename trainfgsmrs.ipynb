{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epoch=60000, imgc=3, imgsz=84, k_qry=15, k_spt=1, meta_lr=0.001, n_way=5, task_num=4, update_lr=0.01, update_step=5, update_step_test=10)\n",
      "=> loading checkpoint 'mamlfgsmeps2_4.pt'\n",
      "=> loaded checkpoint 'mamlfgsmeps2_4.pt' (epoch 1)\n",
      "Meta(\n",
      "  (net): Learner(\n",
      "    conv2d:(ch_in:3, ch_out:32, k:3x3, stride:1, padding:0)\n",
      "    relu:(True,)\n",
      "    bn:(32,)\n",
      "    max_pool2d:(k:2, stride:2, padding:0)\n",
      "    conv2d:(ch_in:32, ch_out:32, k:3x3, stride:1, padding:0)\n",
      "    relu:(True,)\n",
      "    bn:(32,)\n",
      "    max_pool2d:(k:2, stride:2, padding:0)\n",
      "    conv2d:(ch_in:32, ch_out:32, k:3x3, stride:1, padding:0)\n",
      "    relu:(True,)\n",
      "    bn:(32,)\n",
      "    max_pool2d:(k:2, stride:2, padding:0)\n",
      "    conv2d:(ch_in:32, ch_out:32, k:3x3, stride:1, padding:0)\n",
      "    relu:(True,)\n",
      "    bn:(32,)\n",
      "    max_pool2d:(k:2, stride:1, padding:0)\n",
      "    flatten:()\n",
      "    linear:(in:800, out:5)\n",
      "    \n",
      "    (vars): ParameterList(\n",
      "        (0): Parameter containing: [torch.cuda.FloatTensor of size 32x3x3x3 (GPU 2)]\n",
      "        (1): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (2): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (3): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (4): Parameter containing: [torch.cuda.FloatTensor of size 32x32x3x3 (GPU 2)]\n",
      "        (5): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (6): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (7): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (8): Parameter containing: [torch.cuda.FloatTensor of size 32x32x3x3 (GPU 2)]\n",
      "        (9): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (10): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (11): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (12): Parameter containing: [torch.cuda.FloatTensor of size 32x32x3x3 (GPU 2)]\n",
      "        (13): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (14): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (15): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (16): Parameter containing: [torch.cuda.FloatTensor of size 5x800 (GPU 2)]\n",
      "        (17): Parameter containing: [torch.cuda.FloatTensor of size 5 (GPU 2)]\n",
      "    )\n",
      "    (vars_bn): ParameterList(\n",
      "        (0): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (1): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (2): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (3): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (4): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (5): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (6): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "        (7): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 2)]\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total trainable tensors: 32901\n",
      "shuffle DB :train, b:10000, 5-way, 1-shot, 15-query, resize:84\n",
      "shuffle DB :test, b:100, 5-way, 1-shot, 15-query, resize:84\n",
      "step: 0 \ttraining acc: [0.19       0.25666667 0.32333333 0.35666667 0.37333333 0.37666667]\n",
      "step: 0 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.27666667]\n",
      "Test acc: [0.2028 0.2512 0.2856 0.3066 0.3171 0.3232 0.329  0.3315 0.3325 0.3335\n",
      " 0.3335]\n",
      "Test acc_adv: [0.03546 0.05414 0.078   0.09296 0.102   0.10693 0.11066 0.1119  0.11346\n",
      " 0.1144  0.1164 ]\n",
      "Test acc_adv_prior: [0.1545 0.1989 0.2566 0.2903 0.3135 0.322  0.3286 0.331  0.336  0.3372\n",
      " 0.3425]\n",
      "step: 30 \ttraining acc: [0.14       0.27666667 0.32666667 0.34333333 0.35666667 0.38333333]\n",
      "step: 30 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.25333333]\n",
      "step: 60 \ttraining acc: [0.21       0.26666667 0.29666667 0.31333333 0.33333333 0.33666667]\n",
      "step: 60 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.24]\n",
      "step: 90 \ttraining acc: [0.20333333 0.35666667 0.45333333 0.46333333 0.47       0.48      ]\n",
      "step: 90 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.36333333]\n",
      "step: 120 \ttraining acc: [0.25       0.39666667 0.42666667 0.43       0.42       0.41333333]\n",
      "step: 120 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.34666667]\n",
      "step: 150 \ttraining acc: [0.22333333 0.28666667 0.34       0.34333333 0.35333333 0.35666667]\n",
      "step: 150 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.27666667]\n",
      "step: 180 \ttraining acc: [0.16666667 0.23666667 0.24       0.23333333 0.23666667 0.24      ]\n",
      "step: 180 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.15666667]\n",
      "step: 210 \ttraining acc: [0.2        0.32333333 0.38666667 0.41333333 0.42666667 0.44666667]\n",
      "step: 210 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.35]\n",
      "step: 240 \ttraining acc: [0.17333333 0.28333333 0.33666667 0.34666667 0.36666667 0.38      ]\n",
      "step: 240 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.26]\n",
      "step: 270 \ttraining acc: [0.10666667 0.18666667 0.26666667 0.33666667 0.38333333 0.4       ]\n",
      "step: 270 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.24666667]\n",
      "step: 300 \ttraining acc: [0.25       0.33333333 0.35333333 0.37       0.38333333 0.39      ]\n",
      "step: 300 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.27]\n",
      "step: 330 \ttraining acc: [0.2        0.3        0.34333333 0.36       0.38666667 0.39333333]\n",
      "step: 330 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.28]\n",
      "step: 360 \ttraining acc: [0.18333333 0.27666667 0.33333333 0.36666667 0.38333333 0.38666667]\n",
      "step: 360 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.27666667]\n",
      "step: 390 \ttraining acc: [0.22666667 0.31666667 0.36       0.38333333 0.37333333 0.37666667]\n",
      "step: 390 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.25]\n",
      "step: 420 \ttraining acc: [0.17666667 0.31666667 0.35333333 0.34       0.34666667 0.35666667]\n",
      "step: 420 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.25333333]\n",
      "step: 450 \ttraining acc: [0.2        0.3        0.37333333 0.41       0.44       0.43666667]\n",
      "step: 450 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.33333333]\n",
      "step: 480 \ttraining acc: [0.2        0.30666667 0.37       0.40333333 0.40333333 0.42333333]\n",
      "step: 480 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.28666667]\n",
      "1644.3534366799286\n",
      "step: 510 \ttraining acc: [0.20666667 0.34333333 0.41666667 0.46333333 0.49       0.48666667]\n",
      "step: 510 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.34333333]\n",
      "step: 540 \ttraining acc: [0.17333333 0.22666667 0.3        0.33       0.37       0.37333333]\n",
      "step: 540 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.23]\n",
      "step: 570 \ttraining acc: [0.26333333 0.36       0.38       0.37666667 0.38       0.38666667]\n",
      "step: 570 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.25]\n",
      "step: 600 \ttraining acc: [0.24       0.30333333 0.35       0.38       0.4        0.40333333]\n",
      "step: 600 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.27333333]\n",
      "step: 630 \ttraining acc: [0.23333333 0.28333333 0.35       0.36       0.37333333 0.37      ]\n",
      "step: 630 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.27666667]\n",
      "step: 660 \ttraining acc: [0.22666667 0.36       0.42333333 0.43333333 0.42333333 0.41333333]\n",
      "step: 660 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.35]\n",
      "step: 690 \ttraining acc: [0.20333333 0.36666667 0.41666667 0.43       0.45666667 0.46      ]\n",
      "step: 690 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.35333333]\n",
      "step: 720 \ttraining acc: [0.20333333 0.33       0.37666667 0.38       0.37       0.38      ]\n",
      "step: 720 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.29]\n",
      "step: 750 \ttraining acc: [0.22666667 0.38333333 0.44666667 0.45666667 0.46333333 0.46333333]\n",
      "step: 750 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.32333333]\n",
      "step: 780 \ttraining acc: [0.19333333 0.24       0.26333333 0.28666667 0.30666667 0.31      ]\n",
      "step: 780 \ttraining acc_adv: [0.  0.  0.  0.  0.  0.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 810 \ttraining acc: [0.21333333 0.36333333 0.41666667 0.42666667 0.43333333 0.42666667]\n",
      "step: 810 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.33]\n",
      "step: 840 \ttraining acc: [0.25       0.38       0.41       0.45666667 0.45333333 0.45333333]\n",
      "step: 840 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.34]\n",
      "step: 870 \ttraining acc: [0.15666667 0.28       0.34666667 0.42       0.43333333 0.44666667]\n",
      "step: 870 \ttraining acc_adv: [0.  0.  0.  0.  0.  0.3]\n",
      "step: 900 \ttraining acc: [0.2        0.35333333 0.39666667 0.42       0.43       0.43333333]\n",
      "step: 900 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.31666667]\n",
      "step: 930 \ttraining acc: [0.25333333 0.42333333 0.48       0.50333333 0.49333333 0.50666667]\n",
      "step: 930 \ttraining acc_adv: [0.  0.  0.  0.  0.  0.4]\n",
      "step: 960 \ttraining acc: [0.25333333 0.36666667 0.41666667 0.42666667 0.43333333 0.42666667]\n",
      "step: 960 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.30666667]\n",
      "step: 990 \ttraining acc: [0.16666667 0.27333333 0.36666667 0.42       0.46       0.45333333]\n",
      "step: 990 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.34333333]\n",
      "1194.116593610961\n",
      "Test acc: [0.2    0.2644 0.3071 0.3293 0.3384 0.3416 0.3442 0.3467 0.3477 0.3474\n",
      " 0.348 ]\n",
      "Test acc_adv: [0.0508  0.0796  0.0977  0.1137  0.118   0.122   0.1235  0.12384 0.1254\n",
      " 0.126   0.1266 ]\n",
      "Test acc_adv_prior: [0.2461 0.2932 0.3147 0.3376 0.3408 0.352  0.3545 0.3525 0.3564 0.3584\n",
      " 0.3604]\n",
      "step: 1020 \ttraining acc: [0.18333333 0.28333333 0.33       0.36       0.37333333 0.36666667]\n",
      "step: 1020 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.26333333]\n",
      "step: 1050 \ttraining acc: [0.23       0.33666667 0.37666667 0.36333333 0.33666667 0.34666667]\n",
      "step: 1050 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.21666667]\n",
      "step: 1080 \ttraining acc: [0.17       0.3        0.33333333 0.36       0.37       0.36      ]\n",
      "step: 1080 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.26]\n",
      "step: 1110 \ttraining acc: [0.18666667 0.27666667 0.35       0.40333333 0.40666667 0.41      ]\n",
      "step: 1110 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.31666667]\n",
      "step: 1140 \ttraining acc: [0.17       0.3        0.35666667 0.4        0.40333333 0.40666667]\n",
      "step: 1140 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.29333333]\n",
      "step: 1170 \ttraining acc: [0.16333333 0.25       0.30666667 0.36       0.38       0.40333333]\n",
      "step: 1170 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.26666667]\n",
      "step: 1200 \ttraining acc: [0.25       0.35666667 0.44       0.44       0.43333333 0.43333333]\n",
      "step: 1200 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.33666667]\n",
      "step: 1230 \ttraining acc: [0.24       0.33333333 0.37666667 0.38666667 0.40666667 0.40666667]\n",
      "step: 1230 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.28333333]\n",
      "step: 1260 \ttraining acc: [0.17333333 0.21333333 0.25       0.27666667 0.28333333 0.27666667]\n",
      "step: 1260 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.18]\n",
      "step: 1290 \ttraining acc: [0.22333333 0.32333333 0.37333333 0.42       0.44333333 0.43      ]\n",
      "step: 1290 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.30666667]\n",
      "step: 1320 \ttraining acc: [0.20666667 0.26       0.30333333 0.31       0.33333333 0.34      ]\n",
      "step: 1320 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.23333333]\n",
      "step: 1350 \ttraining acc: [0.17       0.29333333 0.35       0.39       0.38666667 0.38      ]\n",
      "step: 1350 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.26666667]\n",
      "step: 1380 \ttraining acc: [0.19333333 0.28       0.33666667 0.36       0.36666667 0.38      ]\n",
      "step: 1380 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.28666667]\n",
      "step: 1410 \ttraining acc: [0.16333333 0.28333333 0.38333333 0.42666667 0.43       0.44666667]\n",
      "step: 1410 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.32666667]\n",
      "step: 1440 \ttraining acc: [0.16333333 0.26333333 0.34       0.35       0.36666667 0.37333333]\n",
      "step: 1440 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.29]\n",
      "step: 1470 \ttraining acc: [0.16333333 0.27666667 0.34666667 0.38333333 0.37       0.38      ]\n",
      "step: 1470 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.23333333]\n",
      "step: 1500 \ttraining acc: [0.18333333 0.31666667 0.38666667 0.41       0.41666667 0.42      ]\n",
      "step: 1500 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.29]\n",
      "step: 1530 \ttraining acc: [0.14333333 0.24       0.35333333 0.38333333 0.39666667 0.41666667]\n",
      "step: 1530 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.29]\n",
      "step: 1560 \ttraining acc: [0.16       0.34333333 0.37666667 0.39333333 0.39       0.38      ]\n",
      "step: 1560 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.26666667]\n",
      "step: 1590 \ttraining acc: [0.25666667 0.34       0.37333333 0.37666667 0.37333333 0.37333333]\n",
      "step: 1590 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.28]\n",
      "step: 1620 \ttraining acc: [0.21666667 0.39333333 0.45333333 0.46666667 0.45666667 0.45333333]\n",
      "step: 1620 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.34]\n",
      "step: 1650 \ttraining acc: [0.25       0.28666667 0.34666667 0.36666667 0.35666667 0.36333333]\n",
      "step: 1650 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.27]\n",
      "step: 1680 \ttraining acc: [0.20666667 0.32666667 0.38333333 0.41666667 0.42       0.43666667]\n",
      "step: 1680 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.31]\n",
      "step: 1710 \ttraining acc: [0.23333333 0.36       0.38       0.39333333 0.38       0.37333333]\n",
      "step: 1710 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.29]\n",
      "step: 1740 \ttraining acc: [0.16       0.25666667 0.31666667 0.37666667 0.42       0.44333333]\n",
      "step: 1740 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.31333333]\n",
      "step: 1770 \ttraining acc: [0.22333333 0.34333333 0.42666667 0.43666667 0.45333333 0.46      ]\n",
      "step: 1770 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.33666667]\n",
      "step: 1800 \ttraining acc: [0.21       0.32       0.35333333 0.38       0.39666667 0.40333333]\n",
      "step: 1800 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.23]\n",
      "step: 1830 \ttraining acc: [0.22666667 0.39666667 0.48       0.51333333 0.52       0.53      ]\n",
      "step: 1830 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.40333333]\n",
      "step: 1860 \ttraining acc: [0.10666667 0.26666667 0.39       0.43       0.44666667 0.46      ]\n",
      "step: 1860 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.32]\n",
      "step: 1890 \ttraining acc: [0.10666667 0.28       0.34       0.38       0.39       0.4       ]\n",
      "step: 1890 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.28333333]\n",
      "step: 1920 \ttraining acc: [0.25       0.37666667 0.45333333 0.45666667 0.46333333 0.46333333]\n",
      "step: 1920 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.31333333]\n",
      "step: 1950 \ttraining acc: [0.21       0.35       0.41       0.44666667 0.45666667 0.46      ]\n",
      "step: 1950 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.35]\n",
      "step: 1980 \ttraining acc: [0.20666667 0.29666667 0.30333333 0.30666667 0.31666667 0.33      ]\n",
      "step: 1980 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.21333333]\n",
      "Test acc: [0.1897 0.278  0.3213 0.3398 0.3462 0.35   0.3525 0.3567 0.3582 0.3604\n",
      " 0.3604]\n",
      "Test acc_adv: [0.0412  0.0715  0.09784 0.10547 0.1129  0.1144  0.11456 0.1148  0.1147\n",
      " 0.11426 0.114  ]\n",
      "Test acc_adv_prior: [0.2072 0.2532 0.297  0.3003 0.3171 0.3184 0.316  0.3132 0.3113 0.3083\n",
      " 0.308 ]\n",
      "step: 2010 \ttraining acc: [0.19333333 0.33666667 0.39333333 0.41       0.41       0.41333333]\n",
      "step: 2010 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.28666667]\n",
      "step: 2040 \ttraining acc: [0.21666667 0.33666667 0.36       0.36666667 0.38333333 0.39333333]\n",
      "step: 2040 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.25333333]\n",
      "step: 2070 \ttraining acc: [0.16666667 0.38666667 0.45666667 0.45       0.48       0.48333333]\n",
      "step: 2070 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.33666667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2100 \ttraining acc: [0.19666667 0.40666667 0.48333333 0.52       0.52       0.53666667]\n",
      "step: 2100 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.41]\n",
      "step: 2130 \ttraining acc: [0.17333333 0.3        0.34       0.35333333 0.35666667 0.36      ]\n",
      "step: 2130 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.23333333]\n",
      "step: 2160 \ttraining acc: [0.17       0.37666667 0.43666667 0.45       0.44666667 0.44333333]\n",
      "step: 2160 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.33333333]\n",
      "step: 2190 \ttraining acc: [0.2        0.36333333 0.46333333 0.47333333 0.48333333 0.49      ]\n",
      "step: 2190 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.33666667]\n",
      "step: 2220 \ttraining acc: [0.26333333 0.40666667 0.42       0.43666667 0.44333333 0.45333333]\n",
      "step: 2220 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.33]\n",
      "step: 2250 \ttraining acc: [0.17       0.28666667 0.36333333 0.38666667 0.40333333 0.40666667]\n",
      "step: 2250 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.29666667]\n",
      "step: 2280 \ttraining acc: [0.17       0.39333333 0.45       0.45       0.46666667 0.47      ]\n",
      "step: 2280 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.33]\n",
      "step: 2310 \ttraining acc: [0.23333333 0.36333333 0.38       0.38       0.37666667 0.37666667]\n",
      "step: 2310 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.24]\n",
      "step: 2340 \ttraining acc: [0.18       0.3        0.37333333 0.4        0.41       0.40666667]\n",
      "step: 2340 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.28]\n",
      "step: 2370 \ttraining acc: [0.17       0.26       0.31666667 0.35       0.36666667 0.36      ]\n",
      "step: 2370 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.21333333]\n",
      "step: 2400 \ttraining acc: [0.22333333 0.37666667 0.43       0.43       0.44333333 0.44666667]\n",
      "step: 2400 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.33333333]\n",
      "step: 2430 \ttraining acc: [0.19       0.28333333 0.35666667 0.37       0.38666667 0.39      ]\n",
      "step: 2430 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.26333333]\n",
      "step: 2460 \ttraining acc: [0.18333333 0.28       0.34666667 0.36333333 0.36666667 0.37      ]\n",
      "step: 2460 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.24666667]\n",
      "step: 2490 \ttraining acc: [0.18       0.37333333 0.42666667 0.44       0.44333333 0.44666667]\n",
      "step: 2490 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.29]\n",
      "step: 0 \ttraining acc: [0.21666667 0.27666667 0.33666667 0.35       0.36333333 0.37333333]\n",
      "step: 0 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.24]\n",
      "Test acc: [0.1975 0.315  0.3523 0.36   0.366  0.3704 0.3713 0.373  0.3738 0.3752\n",
      " 0.3752]\n",
      "Test acc_adv: [0.03265 0.08093 0.10016 0.11    0.1136  0.114   0.1149  0.1164  0.11615\n",
      " 0.11536 0.1147 ]\n",
      "Test acc_adv_prior: [0.1576 0.2429 0.274  0.2961 0.3015 0.2996 0.3    0.3037 0.3027 0.2996\n",
      " 0.2979]\n",
      "step: 30 \ttraining acc: [0.20333333 0.38666667 0.39666667 0.40666667 0.4        0.41      ]\n",
      "step: 30 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.25666667]\n",
      "step: 60 \ttraining acc: [0.19666667 0.43333333 0.45666667 0.46333333 0.46       0.46      ]\n",
      "step: 60 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.34666667]\n",
      "step: 90 \ttraining acc: [0.18666667 0.35       0.36333333 0.37666667 0.38       0.38333333]\n",
      "step: 90 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.28333333]\n",
      "step: 120 \ttraining acc: [0.19666667 0.29333333 0.38333333 0.39       0.42       0.43333333]\n",
      "step: 120 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.29666667]\n",
      "step: 150 \ttraining acc: [0.16333333 0.28666667 0.33       0.37333333 0.39       0.39666667]\n",
      "step: 150 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.26666667]\n",
      "step: 180 \ttraining acc: [0.17       0.30666667 0.33       0.34666667 0.34       0.33333333]\n",
      "step: 180 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.21666667]\n",
      "step: 210 \ttraining acc: [0.24666667 0.41333333 0.45666667 0.46       0.45333333 0.45333333]\n",
      "step: 210 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.34]\n",
      "step: 240 \ttraining acc: [0.21666667 0.37       0.44333333 0.45       0.44666667 0.45666667]\n",
      "step: 240 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.37333333]\n",
      "step: 270 \ttraining acc: [0.2        0.33666667 0.34333333 0.34333333 0.34       0.33333333]\n",
      "step: 270 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.23666667]\n",
      "step: 300 \ttraining acc: [0.17666667 0.39       0.41       0.4        0.38666667 0.39666667]\n",
      "step: 300 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.29]\n",
      "step: 330 \ttraining acc: [0.19666667 0.32333333 0.36666667 0.36333333 0.36       0.36      ]\n",
      "step: 330 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.25666667]\n",
      "step: 360 \ttraining acc: [0.19333333 0.39       0.47       0.46333333 0.48333333 0.48666667]\n",
      "step: 360 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.35333333]\n",
      "step: 390 \ttraining acc: [0.20666667 0.27       0.33666667 0.35333333 0.33666667 0.34666667]\n",
      "step: 390 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.24]\n",
      "step: 420 \ttraining acc: [0.15333333 0.24666667 0.31666667 0.34       0.34       0.35666667]\n",
      "step: 420 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.23333333]\n",
      "step: 450 \ttraining acc: [0.18       0.28       0.38       0.38       0.39666667 0.38666667]\n",
      "step: 450 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.23333333]\n",
      "step: 480 \ttraining acc: [0.28       0.47       0.5        0.52333333 0.51       0.50666667]\n",
      "step: 480 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.36333333]\n",
      "1171.7456264990615\n",
      "step: 510 \ttraining acc: [0.21666667 0.33666667 0.36333333 0.35333333 0.35666667 0.36333333]\n",
      "step: 510 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.24333333]\n",
      "step: 540 \ttraining acc: [0.19666667 0.33666667 0.38666667 0.4        0.4        0.39666667]\n",
      "step: 540 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.24666667]\n",
      "step: 570 \ttraining acc: [0.22333333 0.41333333 0.49       0.5        0.50666667 0.50666667]\n",
      "step: 570 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.37666667]\n",
      "step: 600 \ttraining acc: [0.19666667 0.33666667 0.39666667 0.41333333 0.41666667 0.43333333]\n",
      "step: 600 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.32666667]\n",
      "step: 630 \ttraining acc: [0.10333333 0.36333333 0.46       0.47333333 0.49333333 0.49666667]\n",
      "step: 630 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.34333333]\n",
      "step: 660 \ttraining acc: [0.17666667 0.39333333 0.41666667 0.43666667 0.43666667 0.43333333]\n",
      "step: 660 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.29666667]\n",
      "step: 690 \ttraining acc: [0.25666667 0.48333333 0.50666667 0.52       0.51666667 0.52666667]\n",
      "step: 690 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.38666667]\n",
      "step: 720 \ttraining acc: [0.16333333 0.41       0.49666667 0.48666667 0.50666667 0.50333333]\n",
      "step: 720 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.37666667]\n",
      "step: 750 \ttraining acc: [0.20333333 0.34666667 0.43333333 0.42666667 0.44666667 0.46      ]\n",
      "step: 750 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.29333333]\n",
      "step: 780 \ttraining acc: [0.29333333 0.37       0.42666667 0.43666667 0.43666667 0.44      ]\n",
      "step: 780 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.28]\n",
      "step: 810 \ttraining acc: [0.24666667 0.37333333 0.43333333 0.41666667 0.42333333 0.42666667]\n",
      "step: 810 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.27]\n",
      "step: 840 \ttraining acc: [0.18666667 0.37333333 0.48333333 0.51       0.51666667 0.52333333]\n",
      "step: 840 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.37333333]\n",
      "step: 870 \ttraining acc: [0.21       0.37666667 0.42333333 0.43       0.43666667 0.44333333]\n",
      "step: 870 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.31666667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 900 \ttraining acc: [0.17666667 0.32666667 0.41333333 0.44666667 0.45       0.45      ]\n",
      "step: 900 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.30666667]\n",
      "step: 930 \ttraining acc: [0.27666667 0.44       0.48666667 0.50333333 0.50333333 0.50333333]\n",
      "step: 930 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.36]\n",
      "step: 960 \ttraining acc: [0.18666667 0.43333333 0.46666667 0.49666667 0.50666667 0.51333333]\n",
      "step: 960 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.37333333]\n",
      "step: 990 \ttraining acc: [0.16333333 0.32333333 0.4        0.41666667 0.43       0.44333333]\n",
      "step: 990 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.29333333]\n",
      "1156.2234692079946\n",
      "Test acc: [0.2001 0.3403 0.3728 0.3813 0.386  0.3894 0.3909 0.3904 0.3914 0.3909\n",
      " 0.3909]\n",
      "Test acc_adv: [0.02066 0.07733 0.0977  0.10626 0.1088  0.1095  0.11    0.10986 0.1093\n",
      " 0.1096  0.10986]\n",
      "Test acc_adv_prior: [0.1006 0.2223 0.256  0.2722 0.276  0.2761 0.2761 0.2764 0.2737 0.2751\n",
      " 0.2761]\n",
      "step: 1020 \ttraining acc: [0.17333333 0.45333333 0.50666667 0.52333333 0.53       0.53      ]\n",
      "step: 1020 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.41333333]\n",
      "step: 1050 \ttraining acc: [0.13       0.35666667 0.47666667 0.52666667 0.53666667 0.54333333]\n",
      "step: 1050 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.38666667]\n",
      "step: 1080 \ttraining acc: [0.13       0.26666667 0.32       0.35       0.37       0.38333333]\n",
      "step: 1080 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.23]\n",
      "step: 1110 \ttraining acc: [0.16333333 0.34666667 0.39333333 0.41666667 0.42       0.41666667]\n",
      "step: 1110 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.31]\n",
      "step: 1140 \ttraining acc: [0.19333333 0.39333333 0.42       0.44       0.44666667 0.45666667]\n",
      "step: 1140 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.31]\n",
      "step: 1170 \ttraining acc: [0.15333333 0.31333333 0.37333333 0.37666667 0.38666667 0.38666667]\n",
      "step: 1170 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.24]\n",
      "step: 1200 \ttraining acc: [0.18333333 0.39666667 0.4        0.40333333 0.43       0.43666667]\n",
      "step: 1200 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.27]\n",
      "step: 1230 \ttraining acc: [0.19333333 0.30333333 0.36333333 0.38333333 0.37666667 0.39333333]\n",
      "step: 1230 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.24666667]\n",
      "step: 1260 \ttraining acc: [0.11       0.24666667 0.33333333 0.34333333 0.34333333 0.35666667]\n",
      "step: 1260 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.22]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fb4b381beb5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-fb4b381beb5c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_spt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_spt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_qry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_qry\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scratch/miniconda3/envs/wr/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scratch/miniconda3/envs/wr/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scratch/miniconda3/envs/wr/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/u/scratch/EXPS/EXPSwngr/miniconda3/renwang/MAML/MiniImagenet.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_query_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mquery_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;31m# print(support_set_y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# return support_x, torch.LongTensor(support_y), query_x, torch.LongTensor(query_y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scratch/miniconda3/envs/wr/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/u/scratch/EXPS/EXPSwngr/miniconda3/renwang/MAML/MiniImagenet.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#                                                  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#                                                  ])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             self.transform = transforms.Compose([lambda x: Image.open(x).convert('RGB'),\n\u001b[0m\u001b[1;32m     63\u001b[0m                                                  \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                                                  \u001b[0;31m# transforms.RandomHorizontalFlip(),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scratch/miniconda3/envs/wr/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import  torch, os\n",
    "import  numpy as np\n",
    "from    MiniImagenet import MiniImagenet\n",
    "import  scipy.stats\n",
    "from    torch.utils.data import DataLoader\n",
    "from    torch.optim import lr_scheduler\n",
    "import  random, sys, pickle\n",
    "import  argparse\n",
    "import time\n",
    "\n",
    "#from ANIP import Meta\n",
    "#from metafgsmanil import Meta\n",
    "#from metafgsm import Meta\n",
    "#from MAMLMeta import Meta\n",
    "#from meta import Meta\n",
    "#from Adv_Quer import Meta\n",
    "#from metafgsmnewnew import Meta\n",
    "from metafgsm import Meta\n",
    "\n",
    "\n",
    "def mean_confidence_interval(accs, confidence=0.95):\n",
    "    n = accs.shape[0]\n",
    "    m, se = np.mean(accs), scipy.stats.sem(accs)\n",
    "    h = se * scipy.stats.t._ppf((1 + confidence) / 2, n - 1)\n",
    "    return m, h\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    torch.manual_seed(222)\n",
    "    torch.cuda.manual_seed_all(222)\n",
    "    np.random.seed(222)\n",
    "\n",
    "    print(args)\n",
    "\n",
    "    config = [\n",
    "        ('conv2d', [32, 3, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 1, 0]),\n",
    "        ('flatten', []),\n",
    "        ('linear', [args.n_way, 32 * 5 * 5])\n",
    "    ]\n",
    "\n",
    "    device = torch.device('cuda:2')\n",
    "    maml = Meta(args, config, device).to(device)\n",
    "    \n",
    "    \n",
    "    start_epoch = 0\n",
    "    start_step = 0\n",
    "    filename = 'mamlfgsmeps2_4.pt'\n",
    "    #maml = Meta(args, config).to(device)\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        start_step = checkpoint['step']\n",
    "        maml.net.load_state_dict(checkpoint['state_dict'])\n",
    "        #maml = maml.to(device)\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "    \n",
    "\n",
    "    tmp = filter(lambda x: x.requires_grad, maml.parameters())\n",
    "    num = sum(map(lambda x: np.prod(x.shape), tmp))\n",
    "    print(maml)\n",
    "    print('Total trainable tensors:', num)\n",
    "\n",
    "    # batchsz here means total episode number\n",
    "    mini = MiniImagenet('../../../dataset/', mode='train', n_way=args.n_way, k_shot=args.k_spt,\n",
    "                        k_query=args.k_qry,\n",
    "                        batchsz=10000, resize=args.imgsz)\n",
    "    mini_test = MiniImagenet('../../../dataset/', mode='test', n_way=args.n_way, k_shot=args.k_spt,\n",
    "                             k_query=args.k_qry,\n",
    "                             batchsz=100, resize=args.imgsz)\n",
    "\n",
    "    for epoch in range(args.epoch//10000):\n",
    "        # fetch meta_batchsz num of episode each time\n",
    "        db = DataLoader(mini, args.task_num, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "        for step, (x_spt, y_spt, x_qry, y_qry) in enumerate(db):\n",
    "            if step == 1:\n",
    "                t = time.perf_counter()\n",
    "            if step == 499:\n",
    "                ExecTime = time.perf_counter() - t\n",
    "                print(ExecTime)\n",
    "            if step == 501:\n",
    "                t = time.perf_counter()\n",
    "            if step == 999:\n",
    "                ExecTime = time.perf_counter() - t\n",
    "                print(ExecTime)\n",
    "\n",
    "            x_spt, y_spt, x_qry, y_qry = x_spt.to(device), y_spt.to(device), x_qry.to(device), y_qry.to(device)\n",
    "\n",
    "            accs, accs_adv = maml(x_spt, y_spt, x_qry, y_qry)\n",
    "\n",
    "            if step % 30 == 0:\n",
    "                print('step:', step, '\\ttraining acc:', accs)\n",
    "                print('step:', step, '\\ttraining acc_adv:', accs_adv)\n",
    "                state = {'epoch': epoch, 'step': step, 'state_dict': maml.net.state_dict()}\n",
    "                torch.save(state, 'mamlfgsmeps2_4.pt')\n",
    "\n",
    "            if step % 1000 == 0:  # evaluation\n",
    "                db_test = DataLoader(mini_test, 1, shuffle=True, num_workers=0, pin_memory=True)\n",
    "                accs_all_test = []\n",
    "                accsadv_all_test = []\n",
    "                accsadvpr_all_test = []\n",
    "\n",
    "                for x_spt, y_spt, x_qry, y_qry in db_test:\n",
    "                    x_spt, y_spt, x_qry, y_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                                 x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device)\n",
    "\n",
    "                    accs, accs_adv, accs_adv_prior = maml.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "                    accs_all_test.append(accs)\n",
    "                    accsadv_all_test.append(accs_adv)\n",
    "                    accsadvpr_all_test.append(accs_adv_prior)\n",
    "\n",
    "                # [b, update_step+1]\n",
    "                accs = np.array(accs_all_test).mean(axis=0).astype(np.float16)\n",
    "                accs_adv = np.array(accsadv_all_test).mean(axis=0).astype(np.float16)\n",
    "                accs_adv_prior = np.array(accsadvpr_all_test).mean(axis=0).astype(np.float16)\n",
    "                print('Test acc:', accs)\n",
    "                print('Test acc_adv:', accs_adv)\n",
    "                print('Test acc_adv_prior:', accs_adv_prior)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    argparser = argparse.ArgumentParser()\n",
    "    argparser.add_argument('--epoch', type=int, help='epoch number', default=60000)\n",
    "    argparser.add_argument('--n_way', type=int, help='n way', default=5)\n",
    "    argparser.add_argument('--k_spt', type=int, help='k shot for support set', default=1)\n",
    "    argparser.add_argument('--k_qry', type=int, help='k shot for query set', default=15)\n",
    "    argparser.add_argument('--imgsz', type=int, help='imgsz', default=84)\n",
    "    argparser.add_argument('--imgc', type=int, help='imgc', default=3)\n",
    "    argparser.add_argument('--task_num', type=int, help='meta batch size, namely task num', default=4)\n",
    "    argparser.add_argument('--meta_lr', type=float, help='meta-level outer learning rate', default=1e-3)\n",
    "    argparser.add_argument('--update_lr', type=float, help='task-level inner update learning rate', default=0.01)\n",
    "    argparser.add_argument('--update_step', type=int, help='task-level inner update steps', default=5)\n",
    "    argparser.add_argument('--update_step_test', type=int, help='update steps for finetunning', default=10)\n",
    "    \n",
    "    #argparser.add_argument('--fast', action=\"store_true\", help='whether to use fgsm')\n",
    "\n",
    "    args = argparser.parse_args(args=[])\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
